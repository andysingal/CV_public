# Introduction
BLIP-2, titled "Bridging the Modality Gap," addresses the challenge of crafting a multimodal language model, which typically demands vast computational resources and extensive data. Creating such a model entails working with billions of images, texts, and image-text pairs, which is a highly daunting task.

Rather than constructing the entire architecture from the ground up, BLIP-2 takes a different approach. It forms a bridge called the Q-former that links a pre-trained image encoder with a pre-trained Language Learning Model (LLM).

By capitalizing on pre-existing models, BLIP-2 streamlines its approach. It focuses solely on training the bridge, eliminating the need to train the image encoder and LLM from scratch. This strategy maximizes the utilization of existing technologies and models, optimizing efficiency.

## Resources
- https://101.dev/t/blip-2/892
- https://www.kaggle.com/code/debarshichanda/pytorch-blip-training
- Finetuning-Blip https://github.com/AttentionX/InstructBLIP_PEFT
- https://www.kaggle.com/code/sameen53/instructblip-multi-image-test 

  
<img width="713" alt="Screenshot 2024-02-09 at 9 58 38â€¯AM" src="https://github.com/andysingal/CV_public/assets/20493493/2216180f-e766-40cf-a2fc-a3e1c0b77d8c">
