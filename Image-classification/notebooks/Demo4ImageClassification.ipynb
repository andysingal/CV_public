{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBFwuQ+qAW1+sS9U3BxZHk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pytee/geminiintro/blob/main/Demo4ImageClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i5pkGBI_jdO",
        "outputId": "6ad55d53-b98f-49f0-dd66-fe246c175b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/137.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/137.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "h2woZYJx_lX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "GOOGLE_API_KEY=userdata.get(\"GEMINI_API_KEY\")\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "vz_D3wMm_nBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\"gemini-pro\")"
      ],
      "metadata": {
        "id": "lpusit8t_oMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_markdown(text):\n",
        "    text = text.replace(\"•\", \"  *\")\n",
        "    return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "4rqF8HhA_pYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"Write a image multiclass classification code in pytorch framework using a CIDAR-10 dataset\"\n",
        "    \" I would be training and testing the image classification code in Google colab\",\n",
        "    stream=True\n",
        ")\n",
        "response.resolve()\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "toHwFC7__rLV",
        "outputId": "edb41e26-180e-487d-ca58-ebab1d5d5c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> ```python\n> import torch\n> import torchvision\n> import torch.nn as nn\n> import torch.optim as optim\n> from torchvision import transforms, datasets\n> from torch.utils.data import DataLoader\n> \n> # Define the device\n> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n> \n> # Define the training and testing transforms\n> train_transform = transforms.Compose([\n>     transforms.RandomCrop(32, padding=4),\n>     transforms.RandomHorizontalFlip(),\n>     transforms.ToTensor(),\n>     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n> ])\n> \n> test_transform = transforms.Compose([\n>     transforms.ToTensor(),\n>     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n> ])\n> \n> # Download the CIFAR-10 dataset\n> train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n> test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n> \n> # Create the data loaders\n> train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n> test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n> \n> # Define the model\n> model = torchvision.models.resnet18(pretrained=False)\n> model.fc = nn.Linear(512, 10)\n> model.to(device)\n> \n> # Define the loss function and optimizer\n> criterion = nn.CrossEntropyLoss()\n> optimizer = optim.Adam(model.parameters(), lr=0.001)\n> \n> # Train the model\n> num_epochs = 10\n> for epoch in range(num_epochs):\n>     for i, (images, labels) in enumerate(train_loader):\n>         images = images.to(device)\n>         labels = labels.to(device)\n> \n>         # Forward pass\n>         outputs = model(images)\n>         loss = criterion(outputs, labels)\n> \n>         # Backward pass\n>         optimizer.zero_grad()\n>         loss.backward()\n> \n>         # Update the weights\n>         optimizer.step()\n> \n>         if (i + 1) % 100 == 0:\n>             print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item()}')\n> \n> # Test the model\n> correct = 0\n> total = 0\n> with torch.no_grad():\n>     for images, labels in test_loader:\n>         images = images.to(device)\n>         labels = labels.to(device)\n> \n>         outputs = model(images)\n>         _, predicted = torch.max(outputs.data, 1)\n>         total += labels.size(0)\n>         correct += (predicted == labels).sum().item()\n> \n> print(f'Accuracy of the model on the test images: {100 * correct / total}%')\n> ```"
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the training and testing transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Download the CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "# Create the data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "model = torchvision.models.resnet18(pretrained=False)\n",
        "model.fc = nn.Linear(512, 10)\n",
        "model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item()}')\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the model on the test images: {100 * correct / total}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15_NujtJA2Z2",
        "outputId": "a1e13a01-849f-4b18-cb28-2f52ac51812d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 96073596.50it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/782], Loss: 1.867108941078186\n",
            "Epoch [1/10], Step [200/782], Loss: 1.6108732223510742\n",
            "Epoch [1/10], Step [300/782], Loss: 1.431161880493164\n",
            "Epoch [1/10], Step [400/782], Loss: 1.3744747638702393\n",
            "Epoch [1/10], Step [500/782], Loss: 1.4931761026382446\n",
            "Epoch [1/10], Step [600/782], Loss: 1.373394250869751\n",
            "Epoch [1/10], Step [700/782], Loss: 1.3625941276550293\n",
            "Epoch [2/10], Step [100/782], Loss: 1.3152024745941162\n",
            "Epoch [2/10], Step [200/782], Loss: 1.3129165172576904\n",
            "Epoch [2/10], Step [300/782], Loss: 1.2746528387069702\n",
            "Epoch [2/10], Step [400/782], Loss: 1.2436751127243042\n",
            "Epoch [2/10], Step [500/782], Loss: 1.2316663265228271\n",
            "Epoch [2/10], Step [600/782], Loss: 1.2487694025039673\n",
            "Epoch [2/10], Step [700/782], Loss: 0.9773951768875122\n",
            "Epoch [3/10], Step [100/782], Loss: 0.9363090991973877\n",
            "Epoch [3/10], Step [200/782], Loss: 1.0028033256530762\n",
            "Epoch [3/10], Step [300/782], Loss: 0.778881847858429\n",
            "Epoch [3/10], Step [400/782], Loss: 0.8583905100822449\n",
            "Epoch [3/10], Step [500/782], Loss: 1.0823025703430176\n",
            "Epoch [3/10], Step [600/782], Loss: 0.7362897992134094\n",
            "Epoch [3/10], Step [700/782], Loss: 0.8144421577453613\n",
            "Epoch [4/10], Step [100/782], Loss: 0.9890285730361938\n",
            "Epoch [4/10], Step [200/782], Loss: 0.9285922646522522\n",
            "Epoch [4/10], Step [300/782], Loss: 0.8415253758430481\n",
            "Epoch [4/10], Step [400/782], Loss: 0.8325414657592773\n",
            "Epoch [4/10], Step [500/782], Loss: 0.9986714720726013\n",
            "Epoch [4/10], Step [600/782], Loss: 0.6631445288658142\n",
            "Epoch [4/10], Step [700/782], Loss: 1.0077797174453735\n",
            "Epoch [5/10], Step [100/782], Loss: 0.726042628288269\n",
            "Epoch [5/10], Step [200/782], Loss: 1.0857372283935547\n",
            "Epoch [5/10], Step [300/782], Loss: 0.9091933369636536\n",
            "Epoch [5/10], Step [400/782], Loss: 0.8824754357337952\n",
            "Epoch [5/10], Step [500/782], Loss: 0.7253855466842651\n",
            "Epoch [5/10], Step [600/782], Loss: 0.76465904712677\n",
            "Epoch [5/10], Step [700/782], Loss: 0.9583697319030762\n",
            "Epoch [6/10], Step [100/782], Loss: 0.7375239729881287\n",
            "Epoch [6/10], Step [200/782], Loss: 0.805182933807373\n",
            "Epoch [6/10], Step [300/782], Loss: 0.8819254040718079\n",
            "Epoch [6/10], Step [400/782], Loss: 0.7021761536598206\n",
            "Epoch [6/10], Step [500/782], Loss: 0.67523592710495\n",
            "Epoch [6/10], Step [600/782], Loss: 0.9843491911888123\n",
            "Epoch [6/10], Step [700/782], Loss: 0.6799099445343018\n",
            "Epoch [7/10], Step [100/782], Loss: 0.6445555090904236\n",
            "Epoch [7/10], Step [200/782], Loss: 0.6973190903663635\n",
            "Epoch [7/10], Step [300/782], Loss: 0.6598818302154541\n",
            "Epoch [7/10], Step [400/782], Loss: 0.6572363376617432\n",
            "Epoch [7/10], Step [500/782], Loss: 0.7315282821655273\n",
            "Epoch [7/10], Step [600/782], Loss: 0.8872025012969971\n",
            "Epoch [7/10], Step [700/782], Loss: 0.5378426313400269\n",
            "Epoch [8/10], Step [100/782], Loss: 0.9714867472648621\n",
            "Epoch [8/10], Step [200/782], Loss: 0.7257418036460876\n",
            "Epoch [8/10], Step [300/782], Loss: 0.6902379989624023\n",
            "Epoch [8/10], Step [400/782], Loss: 0.6892907619476318\n",
            "Epoch [8/10], Step [500/782], Loss: 0.8644915819168091\n",
            "Epoch [8/10], Step [600/782], Loss: 0.586513340473175\n",
            "Epoch [8/10], Step [700/782], Loss: 0.8298776745796204\n",
            "Epoch [9/10], Step [100/782], Loss: 0.7072436213493347\n",
            "Epoch [9/10], Step [200/782], Loss: 0.811255156993866\n",
            "Epoch [9/10], Step [300/782], Loss: 0.6209952235221863\n",
            "Epoch [9/10], Step [400/782], Loss: 0.6035144329071045\n",
            "Epoch [9/10], Step [500/782], Loss: 0.5119025707244873\n",
            "Epoch [9/10], Step [600/782], Loss: 0.606617271900177\n",
            "Epoch [9/10], Step [700/782], Loss: 0.8304533958435059\n",
            "Epoch [10/10], Step [100/782], Loss: 0.5963568687438965\n",
            "Epoch [10/10], Step [200/782], Loss: 0.6931501626968384\n",
            "Epoch [10/10], Step [300/782], Loss: 0.8494506478309631\n",
            "Epoch [10/10], Step [400/782], Loss: 0.5403224229812622\n",
            "Epoch [10/10], Step [500/782], Loss: 0.9768345355987549\n",
            "Epoch [10/10], Step [600/782], Loss: 0.6757402420043945\n",
            "Epoch [10/10], Step [700/782], Loss: 0.3573817312717438\n",
            "Accuracy of the model on the test images: 76.94%\n"
          ]
        }
      ]
    }
  ]
}